{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOnmM2Nstszl+kG3PwobRKU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IS2022U/Conversation-Management-Groq/blob/main/yardstick.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pSfVOra-kWJQ"
      },
      "outputs": [],
      "source": [
        "# Installing the dependencies\n",
        "!pip install --quiet openai jsonschema\n",
        "\n",
        "from getpass import getpass\n",
        "import os\n",
        "from openai import OpenAI\n",
        "import json\n",
        "import time\n",
        "from typing import List, Dict\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting API key and base.\n",
        "#This uses getpass so the key is not printed.\n",
        "\n",
        "API_KEY = getpass(\"Enter your Groq/OpenAI-compatible API key : \")\n",
        "API_BASE = getpass(\"Enter the API base (press Enter to use https://api.groq.com/openai/v1): \")\n",
        "if API_BASE.strip() == \"\":\n",
        "    API_BASE = \"https://api.groq.com/openai/v1\"\n",
        "\n",
        "# Save to environment variables\n",
        "os.environ[\"OPENAI_API_KEY\"] = API_KEY\n",
        "os.environ[\"OPENAI_API_BASE\"] = API_BASE\n",
        "\n",
        "# Creating the OpenAI client\n",
        "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"], base_url=os.environ[\"OPENAI_API_BASE\"])\n",
        "\n",
        "MODEL = \"gpt-4o\"\n",
        "\n",
        "print(\"Client created. Model set to:\", MODEL)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Flh8NM7TdZiS",
        "outputId": "67d25bd5-4c30-4363-f3c1-9f320b95a204"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your Groq/OpenAI-compatible API key : Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "Enter the API base (press Enter to use https://api.groq.com/openai/v1): Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "Client created. Model set to: gpt-4o\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#generating the compatible model\n",
        "models = client.models.list()\n",
        "for m in models.data:\n",
        "    print(m.id)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2UU0HqDhiYF",
        "outputId": "1226a87b-d2b3-42bb-dc23-7784742cfc81"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "meta-llama/llama-guard-4-12b\n",
            "deepseek-r1-distill-llama-70b\n",
            "llama-3.3-70b-versatile\n",
            "meta-llama/llama-4-maverick-17b-128e-instruct\n",
            "meta-llama/llama-prompt-guard-2-86m\n",
            "qwen/qwen3-32b\n",
            "groq/compound\n",
            "whisper-large-v3-turbo\n",
            "whisper-large-v3\n",
            "moonshotai/kimi-k2-instruct-0905\n",
            "openai/gpt-oss-20b\n",
            "meta-llama/llama-4-scout-17b-16e-instruct\n",
            "meta-llama/llama-prompt-guard-2-22m\n",
            "playai-tts-arabic\n",
            "gemma2-9b-it\n",
            "openai/gpt-oss-120b\n",
            "playai-tts\n",
            "moonshotai/kimi-k2-instruct\n",
            "llama-3.1-8b-instant\n",
            "allam-2-7b\n",
            "groq/compound-mini\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 1: Conversation Management with Summarization"
      ],
      "metadata": {
        "id": "cEFv4qnPGfML"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConversationManager:\n",
        "    def __init__(self, model: str = \"llama-3.3-70b-versatile\", openai_client=None, k_summarize: int = 3):\n",
        "        \"\"\"\n",
        "        model: model name to use for summarization\n",
        "        openai_client: an instance of OpenAI (required)\n",
        "        k_summarize: perform summarization after every k runs\n",
        "        \"\"\"\n",
        "        if openai_client is None:\n",
        "            raise ValueError(\"openai_client must be provided (pass the OpenAI client instance)\")\n",
        "        self.model = model\n",
        "        self.openai = openai_client\n",
        "        self.history: list[dict] = []\n",
        "        self.run_count = 0\n",
        "        self.k_summarize = k_summarize\n",
        "\n",
        "    # basic history operations\n",
        "    def add_message(self, role: str, content: str):\n",
        "        assert role in (\"user\", \"assistant\", \"system\")\n",
        "        self.history.append({\"role\": role, \"content\": content})\n",
        "\n",
        "    def get_history(self) -> list[dict]:\n",
        "        return self.history.copy()\n",
        "\n",
        "    # truncation helpers\n",
        "    def truncate_by_turns(self, n: int):\n",
        "        if n <= 0:\n",
        "            return []\n",
        "        return self.history[-n:]\n",
        "\n",
        "    def truncate_by_chars(self, max_chars: int):\n",
        "        if max_chars <= 0:\n",
        "            return []\n",
        "        kept = []\n",
        "        total = 0\n",
        "        for m in reversed(self.history):\n",
        "            if total + len(m['content']) > max_chars:\n",
        "                break\n",
        "            kept.append(m)\n",
        "            total += len(m['content'])\n",
        "        return list(reversed(kept))\n",
        "\n",
        "    def truncate_by_words(self, max_words: int):\n",
        "        if max_words <= 0:\n",
        "            return []\n",
        "        kept = []\n",
        "        total = 0\n",
        "        for m in reversed(self.history):\n",
        "            wc = len(m['content'].split())\n",
        "            if total + wc > max_words:\n",
        "                break\n",
        "            kept.append(m)\n",
        "            total += wc\n",
        "        return list(reversed(kept))\n",
        "\n",
        "    # summarization\n",
        "    def _build_summarization_prompt(self, messages: list[dict]) -> str:\n",
        "        conversation_text = \"\"\n",
        "        for m in messages:\n",
        "            conversation_text += f\"{m['role'].upper()}: {m['content']}\\n\"\n",
        "        prompt = (\n",
        "            \"You are a concise summarizer. Summarize the conversation into:\\n\"\n",
        "            \"- A short 2-4 line summary\\n\"\n",
        "            \"- Bullet points with key facts / user requests\\n\"\n",
        "            \"- Any actions or pending items (if present)\\n\\n\"\n",
        "            \"Conversation:\\n\"\n",
        "            f\"{conversation_text}\\n\"\n",
        "            \"Return only the summary (no extra commentary).\"\n",
        "        )\n",
        "        return prompt\n",
        "\n",
        "    def summarize_messages(self, messages: list[dict], max_tokens: int = 256) -> str:\n",
        "        prompt = self._build_summarization_prompt(messages)\n",
        "        try:\n",
        "            resp = self.openai.chat.completions.create(\n",
        "                model=self.model,\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                max_tokens=max_tokens,\n",
        "                temperature=0.2,\n",
        "            )\n",
        "            # Access the message correctly\n",
        "            summary = resp.choices[0].message.content.strip()\n",
        "            return summary\n",
        "        except Exception as e:\n",
        "            print(\"Summarization failed:\", e)\n",
        "            return \"[SUMMARY FAILED]\"\n",
        "\n",
        "    def maybe_summarize(self):\n",
        "        self.run_count += 1\n",
        "        if self.k_summarize <= 0 or (self.run_count % self.k_summarize != 0):\n",
        "            return False\n",
        "\n",
        "        n_keep_recent = 2\n",
        "        if len(self.history) <= n_keep_recent + 1:\n",
        "            return False\n",
        "\n",
        "        messages_to_summarize = self.history[:-n_keep_recent]\n",
        "        summary_text = self.summarize_messages(messages_to_summarize)\n",
        "        summary_message = {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": f\"[SUMMARY OF EARLIER CONVERSATION]\\n{summary_text}\",\n",
        "        }\n",
        "        self.history = [summary_message] + self.history[-n_keep_recent:]\n",
        "        return True\n",
        "\n",
        "    # Convenience: get truncated view\n",
        "    def get_truncated_view(self, limit_turns=None, limit_chars=None, limit_words=None):\n",
        "        view = self.history\n",
        "        if limit_turns is not None:\n",
        "            view = view[-limit_turns:]\n",
        "        if limit_chars is not None:\n",
        "            view = self.truncate_by_chars(limit_chars)\n",
        "        if limit_words is not None:\n",
        "            view = self.truncate_by_words(limit_words)\n",
        "        return view\n",
        "\n",
        "    # pretty-print\n",
        "    def pretty_print(self, msgs=None):\n",
        "        if msgs is None:\n",
        "            msgs = self.history\n",
        "        for i, m in enumerate(msgs, start=1):\n",
        "            print(f\"[{i}] {m['role'].upper()}: {m['content']}\\n\")\n"
      ],
      "metadata": {
        "id": "k6f1HLzmkZl9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Demonstration with sample conversation:"
      ],
      "metadata": {
        "id": "Ylw8R_TDG2wR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cm = ConversationManager(model=\"llama-3.3-70b-versatile\", openai_client=client, k_summarize=3)\n",
        "\n",
        "cm.add_message(\"user\", \"Hi, I'm looking to book a meeting next week.\")\n",
        "cm.add_message(\"assistant\", \"Sure â€” what days/times work for you?\")\n",
        "cm.maybe_summarize()\n",
        "\n",
        "cm.add_message(\"user\", \"Any chance for Tuesday or Wednesday morning?\")\n",
        "cm.add_message(\"assistant\", \"I have openings on Tuesday 10am and Wednesday 9am.\")\n",
        "cm.maybe_summarize()\n",
        "\n",
        "cm.add_message(\"user\", \"Tuesday 10am works. Also, remind me to prepare the report.\")\n",
        "cm.add_message(\"assistant\", \"Noted â€” meeting set for Tue 10am. Will remind you to prepare the report.\")\n",
        "summed = cm.maybe_summarize()\n",
        "print(\"summarized?\", summed)\n",
        "cm.pretty_print()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qv5to3wL6pdq",
        "outputId": "6b81941e-b9cd-4e6e-e463-a255ce2d7a80"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "summarized? True\n",
            "[1] SYSTEM: [SUMMARY OF EARLIER CONVERSATION]\n",
            "User is booking a meeting for next week, with preferred days being Tuesday or Wednesday morning. \n",
            "Available slots are Tuesday 10am and Wednesday 9am.\n",
            "* User wants to book a meeting next week\n",
            "* Preferred days: Tuesday or Wednesday morning\n",
            "* Available meeting times: Tuesday 10am, Wednesday 9am\n",
            "* Action: User to confirm meeting time\n",
            "\n",
            "[2] USER: Tuesday 10am works. Also, remind me to prepare the report.\n",
            "\n",
            "[3] ASSISTANT: Noted â€” meeting set for Tue 10am. Will remind you to prepare the report.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Last 3 turns:\")\n",
        "cm.pretty_print(cm.get_truncated_view(limit_turns=3))\n",
        "\n",
        "print(\"\\nTruncated to 150 chars:\")\n",
        "cm.pretty_print(cm.get_truncated_view(limit_chars=150))\n",
        "\n",
        "print(\"\\nTruncated to 30 words:\")\n",
        "cm.pretty_print(cm.get_truncated_view(limit_words=30))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWvdR1zS67S8",
        "outputId": "761295f4-5a36-41eb-b95b-818e64212d10"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last 3 turns:\n",
            "[1] SYSTEM: [SUMMARY OF EARLIER CONVERSATION]\n",
            "User is booking a meeting for next week, with preferred days being Tuesday or Wednesday morning. \n",
            "Available slots are Tuesday 10am and Wednesday 9am.\n",
            "* User wants to book a meeting next week\n",
            "* Preferred days: Tuesday or Wednesday morning\n",
            "* Available meeting times: Tuesday 10am, Wednesday 9am\n",
            "* Action: User to confirm meeting time\n",
            "\n",
            "[2] USER: Tuesday 10am works. Also, remind me to prepare the report.\n",
            "\n",
            "[3] ASSISTANT: Noted â€” meeting set for Tue 10am. Will remind you to prepare the report.\n",
            "\n",
            "\n",
            "Truncated to 150 chars:\n",
            "[1] USER: Tuesday 10am works. Also, remind me to prepare the report.\n",
            "\n",
            "[2] ASSISTANT: Noted â€” meeting set for Tue 10am. Will remind you to prepare the report.\n",
            "\n",
            "\n",
            "Truncated to 30 words:\n",
            "[1] USER: Tuesday 10am works. Also, remind me to prepare the report.\n",
            "\n",
            "[2] ASSISTANT: Noted â€” meeting set for Tue 10am. Will remind you to prepare the report.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create ConversationManager instance with model llama to analyze the dsifferences\n",
        "cm = ConversationManager(model=\"llama-3.3-70b-versatile\", openai_client=client, k_summarize=3)\n",
        "\n",
        "# Run 1\n",
        "cm.add_message(\"user\", \"Hello! I need help planning my weekend trip.\")\n",
        "cm.add_message(\"assistant\", \"Sure! Where are you planning to go, and how many days do you have?\")\n",
        "cm.maybe_summarize()  # run_count = 1 -> no summarization\n",
        "\n",
        "# Run 2\n",
        "cm.add_message(\"user\", \"I'm thinking of going to Pokhara for 2 days.\")\n",
        "cm.add_message(\"assistant\", \"Great! Do you prefer adventure activities, sightseeing, or relaxation?\")\n",
        "cm.maybe_summarize()  # run_count = 2 -> no summarization\n",
        "\n",
        "# Run 3\n",
        "cm.add_message(\"user\", \"I want a mix of sightseeing and some light trekking.\")\n",
        "cm.add_message(\"assistant\", \"Perfect! I suggest visiting Phewa Lake, World Peace Pagoda, and doing a short trek to Sarangkot.\")\n",
        "summed = cm.maybe_summarize()  # run_count = 3 -> summarization occurs\n",
        "print(\"summarized?\", summed)\n",
        "cm.pretty_print()\n",
        "\n",
        "# Demonstrate truncation options\n",
        "print(\"Last 3 turns:\")\n",
        "cm.pretty_print(cm.get_truncated_view(limit_turns=3))\n",
        "\n",
        "print(\"\\nTruncated to 150 chars:\")\n",
        "cm.pretty_print(cm.get_truncated_view(limit_chars=150))\n",
        "\n",
        "print(\"\\nTruncated to 30 words:\")\n",
        "cm.pretty_print(cm.get_truncated_view(limit_words=30))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqD4jJQSlimz",
        "outputId": "5e4a6b9b-151a-4453-d810-58b8d9f34e11"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "summarized? True\n",
            "[1] SYSTEM: [SUMMARY OF EARLIER CONVERSATION]\n",
            "User is planning a 2-day weekend trip to Pokhara. They are considering options for activities. \n",
            "* Destination: Pokhara\n",
            "* Duration: 2 days\n",
            "No actions or pending items.\n",
            "\n",
            "[2] USER: I want a mix of sightseeing and some light trekking.\n",
            "\n",
            "[3] ASSISTANT: Perfect! I suggest visiting Phewa Lake, World Peace Pagoda, and doing a short trek to Sarangkot.\n",
            "\n",
            "Last 3 turns:\n",
            "[1] SYSTEM: [SUMMARY OF EARLIER CONVERSATION]\n",
            "User is planning a 2-day weekend trip to Pokhara. They are considering options for activities. \n",
            "* Destination: Pokhara\n",
            "* Duration: 2 days\n",
            "No actions or pending items.\n",
            "\n",
            "[2] USER: I want a mix of sightseeing and some light trekking.\n",
            "\n",
            "[3] ASSISTANT: Perfect! I suggest visiting Phewa Lake, World Peace Pagoda, and doing a short trek to Sarangkot.\n",
            "\n",
            "\n",
            "Truncated to 150 chars:\n",
            "[1] USER: I want a mix of sightseeing and some light trekking.\n",
            "\n",
            "[2] ASSISTANT: Perfect! I suggest visiting Phewa Lake, World Peace Pagoda, and doing a short trek to Sarangkot.\n",
            "\n",
            "\n",
            "Truncated to 30 words:\n",
            "[1] USER: I want a mix of sightseeing and some light trekking.\n",
            "\n",
            "[2] ASSISTANT: Perfect! I suggest visiting Phewa Lake, World Peace Pagoda, and doing a short trek to Sarangkot.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 2: JSON Schema Classification & Information Extraction"
      ],
      "metadata": {
        "id": "yoNBabB17KJa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import re\n",
        "import random\n",
        "from getpass import getpass\n",
        "from typing import Optional, Callable, Any, Dict\n",
        "\n",
        "# jsonschema for validation\n",
        "from jsonschema import validate, ValidationError\n",
        "\n",
        "from openai import OpenAI\n",
        "\n",
        "# Get API key without echoing\n",
        "API_KEY = getpass(\"Enter your Groq/OpenAI-compatible API key: \")\n",
        "os.environ[\"OPENAI_API_KEY\"] = API_KEY\n",
        "\n",
        "# Set Groq API base (use the endpoint you confirmed works).\n",
        "# Common values: https://api.groq.com/openai/v1  or https://api.groq.com/v1\n",
        "os.environ[\"OPENAI_API_BASE\"] = \"https://api.groq.com/openai/v1\"\n",
        "\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "MODEL = \"gpt-4o-mini\"\n",
        "\n",
        "try:\n",
        "    models = client.models.list()\n",
        "    print(\"Authenticated â€” top models (first 8):\", [m.id for m in models.data[:8]])\n",
        "except Exception as e:\n",
        "    print(\"Authentication check FAILED:\", str(e))\n",
        "    # If this fails, stop here and verify API_BASE and key.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBty324WEiCB",
        "outputId": "7e4bc98f-bd79-4711-c1c3-927c73720bda"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your Groq/OpenAI-compatible API key: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "Authenticated â€” top models (first 8): ['gpt-3.5-turbo', 'gpt-audio', 'gpt-5-mini', 'gpt-5-nano-2025-08-07', 'gpt-5-nano', 'gpt-audio-2025-08-28', 'davinci-002', 'babbage-002']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Schema + function description\n",
        "\n",
        "schema = {\n",
        "    \"type\": \"object\",\n",
        "    \"properties\": {\n",
        "        \"name\": {\"type\": \"string\", \"description\": \"Full name of the person\"},\n",
        "        \"email\": {\"type\": \"string\", \"format\": \"email\", \"description\": \"Email address\"},\n",
        "        \"phone\": {\"type\": \"string\", \"description\": \"Phone number in any reasonable format\"},\n",
        "        \"location\": {\"type\": \"string\", \"description\": \"City/country or full address\"},\n",
        "        \"age\": {\"type\": \"integer\", \"minimum\": 0, \"maximum\": 120, \"description\": \"Person's age in years\"}\n",
        "    },\n",
        "    \"required\": [\"name\"],\n",
        "    \"additionalProperties\": False\n",
        "}\n",
        "\n",
        "functions = [\n",
        "    {\n",
        "        \"name\": \"extract_user_info\",\n",
        "        \"description\": \"Extract name, email, phone, location and age from the user's message in JSON format.\",\n",
        "        \"parameters\": schema\n",
        "    }\n",
        "]\n",
        "\n"
      ],
      "metadata": {
        "id": "aQW2ujkPyO0f"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retry decorator and helper to handle error due to rate limit in api\n",
        "\n",
        "def is_rate_limit_error(exc: Exception) -> bool:\n",
        "    \"\"\"Return True if exception likely indicates a rate limit (429) or retryable throttling.\"\"\"\n",
        "\n",
        "    msg = str(exc).lower()\n",
        "    if \"rate limit\" in msg or \"too many requests\" in msg or \"429\" in msg:\n",
        "        return True\n",
        "\n",
        "    sc = getattr(exc, \"status_code\", None) or getattr(exc, \"http_status\", None)\n",
        "    if sc == 429:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def retry_on_rate_limit(max_attempts: int = 6, backoff_base: float = 1.0, max_backoff: float = 30.0):\n",
        "    \"\"\"Decorator to retry a function on rate-limit errors with exponential backoff + jitter.\"\"\"\n",
        "    def decorator(fn: Callable):\n",
        "        def wrapper(*args, **kwargs):\n",
        "            attempt = 0\n",
        "            while True:\n",
        "                try:\n",
        "                    return fn(*args, **kwargs)\n",
        "                except Exception as e:\n",
        "                    attempt += 1\n",
        "                    if attempt >= max_attempts or not is_rate_limit_error(e):\n",
        "\n",
        "                        raise\n",
        "\n",
        "                    backoff = min(max_backoff, backoff_base * (2 ** (attempt - 1)))\n",
        "                    jitter = random.uniform(0, backoff * 0.1)\n",
        "                    sleep_for = backoff + jitter\n",
        "                    print(f\"[retry_on_rate_limit] Rate limit detected. Attempt {attempt}/{max_attempts}. Sleeping {sleep_for:.1f}s and retrying...\")\n",
        "                    time.sleep(sleep_for)\n",
        "        return wrapper\n",
        "    return decorator\n",
        "\n",
        "# small helper to normalize phone numbers (very simple; adjust for your needs)\n",
        "def normalize_phone(phone_str: str) -> Optional[str]:\n",
        "    if not phone_str:\n",
        "        return None\n",
        "    s = re.sub(r\"[^\\d\\+]\", \"\", phone_str)  # in order to keep digits and plus\n",
        "\n",
        "    if len(re.sub(r\"\\D\", \"\", s)) >= 7:\n",
        "        return s\n",
        "    return None\n",
        "\n"
      ],
      "metadata": {
        "id": "NlDpuzDm7cqh"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main extraction function\n",
        "@retry_on_rate_limit(max_attempts=6, backoff_base=1.0, max_backoff=20.0)\n",
        "def extract_info_from_text(text: str, model: Optional[str] = None) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Sends `text` to the model using function-calling and validates result against the JSON schema.\n",
        "    Returns a dict with raw response, parsed dict, validation flags and normalized fields.\n",
        "    \"\"\"\n",
        "    if model is None:\n",
        "        model = MODEL\n",
        "\n",
        "    messages = [{\"role\": \"user\", \"content\": text}]\n",
        "\n",
        "    # Call chat completion / function-calling\n",
        "    resp = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        functions=functions,\n",
        "        function_call=\"auto\",\n",
        "        temperature=0.0,\n",
        "        max_tokens=512\n",
        "    )\n",
        "\n",
        "\n",
        "    choice = resp.choices[0]\n",
        "    message = getattr(choice, \"message\", None) or choice  # some clients different\n",
        "\n",
        "    parsed = {}\n",
        "    raw_args = None\n",
        "\n",
        "\n",
        "    func_call = getattr(message, \"function_call\", None)\n",
        "    if func_call and getattr(func_call, \"arguments\", None):\n",
        "        raw_args = func_call.arguments\n",
        "    else:\n",
        "        raw_args = getattr(message, \"content\", \"\")\n",
        "\n",
        "    # Parse JSON\n",
        "    if raw_args:\n",
        "        try:\n",
        "            parsed = json.loads(raw_args)\n",
        "        except Exception:\n",
        "            # fallback: try to extract JSON substring\n",
        "            txt = raw_args\n",
        "            json_match = re.search(r\"(\\{[\\s\\S]*\\})\", txt)\n",
        "            if json_match:\n",
        "                try:\n",
        "                    parsed = json.loads(json_match.group(1))\n",
        "                except Exception:\n",
        "                    parsed = {}\n",
        "            else:\n",
        "                parsed = {}\n",
        "\n",
        "    # Normalize / coerce fields\n",
        "    # Age might be string like \"twenty\" or \"22 years\" etc. Try extract digits first.\n",
        "    age = parsed.get(\"age\")\n",
        "    if isinstance(age, str):\n",
        "        # extract first integer found\n",
        "        m = re.search(r\"(\\d{1,3})\", age)\n",
        "        if m:\n",
        "            parsed[\"age\"] = int(m.group(1))\n",
        "        else:\n",
        "            # attempt textual number -> not implemented; set None so validation fails later\n",
        "            parsed[\"age\"] = None\n",
        "\n",
        "    # phone normalization\n",
        "    if \"phone\" in parsed:\n",
        "        parsed[\"phone\"] = normalize_phone(parsed[\"phone\"])\n",
        "\n",
        "    # Ensure keys exist (explicitly set None if missing)\n",
        "    for k in [\"name\", \"email\", \"phone\", \"location\", \"age\"]:\n",
        "        parsed.setdefault(k, None)\n",
        "\n",
        "    # Validate against schema using jsonschema\n",
        "    try:\n",
        "        validate(instance=parsed, schema=schema)\n",
        "        valid = True\n",
        "        validation_error = None\n",
        "    except ValidationError as ve:\n",
        "        valid = False\n",
        "        validation_error = str(ve)\n",
        "\n",
        "    return {\n",
        "        \"raw_model_output\": resp,\n",
        "        \"parsed\": parsed,\n",
        "        \"valid\": valid,\n",
        "        \"validation_error\": validation_error\n",
        "    }\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8xHSXQhebdp-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# local regex-based fallback extractor ===\n",
        "\n",
        "# If phonenumbers is not installed, the code below will still work using simpler regex normalization.\n",
        "\n",
        "number_word_map = {\n",
        "    \"zero\":0,\"one\":1,\"two\":2,\"three\":3,\"four\":4,\"five\":5,\"six\":6,\"seven\":7,\"eight\":8,\"nine\":9,\n",
        "    \"ten\":10,\"eleven\":11,\"twelve\":12,\"thirteen\":13,\"fourteen\":14,\"fifteen\":15,\"sixteen\":16,\n",
        "    \"seventeen\":17,\"eighteen\":18,\"nineteen\":19,\"twenty\":20,\"twenty one\":21,\"twenty-two\":22,\n",
        "    \"twenty three\":23,\"twenty four\":24,\"twenty five\":25\n",
        "}\n",
        "\n",
        "def try_parse_age(s):\n",
        "    if not s: return None\n",
        "    # extract digits\n",
        "    m = re.search(r\"\\b(\\d{1,3})\\b\", s)\n",
        "    if m:\n",
        "        val = int(m.group(1))\n",
        "        if 0 <= val <= 120:\n",
        "            return val\n",
        "    # try to map words (simple)\n",
        "    s2 = s.lower().strip()\n",
        "    return number_word_map.get(s2)\n",
        "\n",
        "def normalize_phone_simple(s):\n",
        "    if not s: return None\n",
        "    digits = re.sub(r\"[^\\d\\+]\", \"\", s)\n",
        "    if len(re.sub(r\"\\D\",\"\", digits)) >= 7:\n",
        "        return digits\n",
        "    return None\n",
        "\n",
        "def fallback_extract(text):\n",
        "    out = {\"name\": None, \"email\": None, \"phone\": None, \"location\": None, \"age\": None}\n",
        "    # name heuristics(look for \"I'm X\", \"I am X\", \"this is X\", \"name: X\")\n",
        "    m = re.search(r\"(?:i'm|i am|this is|my name is)\\s+([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*)\", text, re.I)\n",
        "    if m:\n",
        "        out[\"name\"] = m.group(1).strip()\n",
        "    else:\n",
        "        m2 = re.search(r\"name[:\\-]\\s*([A-Z][\\w\\s\\.]+)\", text)\n",
        "        if m2:\n",
        "            out[\"name\"] = m2.group(1).strip()\n",
        "    # email\n",
        "    m = re.search(r\"([a-zA-Z0-9._%+\\-]+@[a-zA-Z0-9.\\-]+\\.[a-zA-Z]{2,})\", text)\n",
        "    if m:\n",
        "        out[\"email\"] = m.group(1).strip()\n",
        "    # phone\n",
        "    m = re.search(r\"(\\+?\\d{2,4}[\\-\\s]?\\d{6,12}|\\b\\d{7,12}\\b)\", text)\n",
        "    if m:\n",
        "        out[\"phone\"] = normalize_phone_simple(m.group(1))\n",
        "    # location\n",
        "    m = re.search(r\"(?:in|at|near|location[:\\-])\\s+([A-Z][\\w\\s]+)\", text)\n",
        "    if m:\n",
        "        out[\"location\"] = m.group(1).strip().rstrip(\".\")\n",
        "    # age\n",
        "    m = re.search(r\"\\b(age|I'm|I am|i'm)\\s*[:\\-]?\\s*(\\d{1,3})\\b\", text, re.I)\n",
        "    if m:\n",
        "        out[\"age\"] = try_parse_age(m.group(2))\n",
        "    else:\n",
        "        #  for phrases like \"I'm 22 years old\" or \"Age: 28\"\n",
        "        m2 = re.search(r\"\\b(\\d{1,3})\\s*(?:years?|yrs?)\\b\", text, re.I)\n",
        "        if m2:\n",
        "            out[\"age\"] = try_parse_age(m2.group(1))\n",
        "        else:\n",
        "            # try textual\n",
        "            for word_form in number_word_map.keys():\n",
        "                if word_form in text.lower():\n",
        "                    out[\"age\"] = number_word_map[word_form]\n",
        "                    break\n",
        "    # ensure at least name exists otherwise None\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "jjYvkkQlhDap"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Updated wrapper: try API, fall back to fallback_extract\n",
        "MODEL = \"gpt-4o-mini\"\n",
        "\n",
        "def is_insufficient_quota(exc: Exception) -> bool:\n",
        "    \"\"\"\n",
        "    Heuristic to detect quota / billing errors from exception text.\n",
        "    Adjust the keywords to fit the exact messages you see from the Groq/OpenAI client.\n",
        "    \"\"\"\n",
        "    msg = str(exc).lower()\n",
        "    keywords = [\"insufficient_quota\", \"quota\", \"billing\", \"insufficient credits\", \"payment required\", \"limit exceeded\"]\n",
        "    return any(k in msg for k in keywords)\n",
        "\n",
        "@retry_on_rate_limit(max_attempts=4)\n",
        "def call_api_extract(text):\n",
        "    messages = [{\"role\": \"user\", \"content\": text}]\n",
        "    resp = client.chat.completions.create(\n",
        "        model=MODEL,\n",
        "        messages=messages,\n",
        "        functions=functions,\n",
        "        function_call=\"auto\",\n",
        "        temperature=0.0,\n",
        "        max_tokens=400\n",
        "    )\n",
        "\n",
        "    # try to parse function_call or content\n",
        "    choice = resp.choices[0]\n",
        "    # some SDKs put the message under choice.message, others under the choice object directly\n",
        "    message = getattr(choice, \"message\", None) or choice\n",
        "    parsed = {}\n",
        "    raw_args = None\n",
        "\n",
        "    # function_call.arguments can be a JSON string or already-parsed dict in some SDKs\n",
        "    fc = getattr(message, \"function_call\", None)\n",
        "    if fc and getattr(fc, \"arguments\", None):\n",
        "        raw_args = fc.arguments\n",
        "    else:\n",
        "        # fallback to content (string)\n",
        "        raw_args = getattr(message, \"content\", \"\") or \"\"\n",
        "\n",
        "    # If raw_args is already a dict, use it directly; otherwise try to load JSON\n",
        "    if isinstance(raw_args, dict):\n",
        "        parsed = raw_args\n",
        "    else:\n",
        "        # Expecting a JSON string; try direct load, else extract JSON substring\n",
        "        try:\n",
        "            parsed = json.loads(raw_args) if raw_args else {}\n",
        "        except Exception:\n",
        "            m = re.search(r\"(\\{[\\s\\S]*\\})\", str(raw_args))\n",
        "            if m:\n",
        "                try:\n",
        "                    parsed = json.loads(m.group(1))\n",
        "                except Exception:\n",
        "                    parsed = {}\n",
        "            else:\n",
        "                parsed = {}\n",
        "\n",
        "    # coerce age if string\n",
        "    if \"age\" in parsed and isinstance(parsed[\"age\"], str):\n",
        "        a = re.search(r\"(\\d{1,3})\", parsed[\"age\"])\n",
        "        if a:\n",
        "            parsed[\"age\"] = int(a.group(1))\n",
        "        else:\n",
        "            parsed[\"age\"] = None\n",
        "\n",
        "    # normalize phone minimal (assumes normalize_phone_simple exists)\n",
        "    if \"phone\" in parsed and parsed[\"phone\"] is not None:\n",
        "        parsed[\"phone\"] = normalize_phone_simple(parsed[\"phone\"])\n",
        "\n",
        "    # ensure keys exist\n",
        "    for k in [\"name\", \"email\", \"phone\", \"location\", \"age\"]:\n",
        "        parsed.setdefault(k, None)\n",
        "\n",
        "    # validate against schema\n",
        "    try:\n",
        "        validate(instance=parsed, schema=schema)\n",
        "        valid = True\n",
        "    except ValidationError:\n",
        "        valid = False\n",
        "\n",
        "    return {\"from\": \"api\", \"parsed\": parsed, \"valid\": valid, \"raw\": resp}\n",
        "\n",
        "\n",
        "def extract_info_with_fallback(text):\n",
        "    \"\"\"\n",
        "    Try API extraction first. On exceptions (connectivity, quota), fall back to local extractor.\n",
        "    Returns a dict with consistent keys:\n",
        "      - from: \"api\" or \"fallback\"\n",
        "      - parsed: dict\n",
        "      - valid: bool (schema validation result)\n",
        "      - raw: API response object (if from api) else None\n",
        "      - error: error string if fallback was used\n",
        "    \"\"\"\n",
        "    try:\n",
        "        out = call_api_extract(text)\n",
        "        return out\n",
        "    except Exception as e:\n",
        "        # Decide if this is a quota/billing error (so we shouldn't retry further)\n",
        "        if is_insufficient_quota(e):\n",
        "            print(\"[extract_info_with_fallback] Insufficient quota detected â€” using local fallback extractor.\")\n",
        "        else:\n",
        "            print(f\"[extract_info_with_fallback] API call failed ({e}) â€” using local fallback extractor.\")\n",
        "\n",
        "        # Local fallback extractor should return a dict-like parsed result\n",
        "        parsed = fallback_extract(text)\n",
        "\n",
        "        # Ensure keys exist in fallback output too\n",
        "        for k in [\"name\", \"email\", \"phone\", \"location\", \"age\"]:\n",
        "            parsed.setdefault(k, None)\n",
        "\n",
        "        # Apply same normalization used for API path\n",
        "        if isinstance(parsed.get(\"age\"), str):\n",
        "            a = re.search(r\"(\\d{1,3})\", parsed[\"age\"])\n",
        "            parsed[\"age\"] = int(a.group(1)) if a else None\n",
        "\n",
        "        if parsed.get(\"phone\") is not None:\n",
        "            parsed[\"phone\"] = normalize_phone_simple(parsed[\"phone\"])\n",
        "\n",
        "        # Validate fallback parsed result\n",
        "        try:\n",
        "            validate(instance=parsed, schema=schema)\n",
        "            valid = True\n",
        "        except ValidationError:\n",
        "            valid = False\n",
        "\n",
        "        return {\"from\": \"fallback\", \"parsed\": parsed, \"valid\": valid, \"raw\": None, \"error\": str(e)}\n",
        "\n"
      ],
      "metadata": {
        "id": "0FMtMgn7hfZD"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Demonstrate with 3 sample chats"
      ],
      "metadata": {
        "id": "JeDtjOH07dHI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  demo run\n",
        "sample_chats = [\n",
        "    \"Hi, I'm Isu Sharma. My email is isu.sharma@example.com and phone is +977-9812345678. I live in Kathmandu. I'm 22 years old.\",\n",
        "    \"Hello â€” name: Sita Sharma; contact: sita_sharma@gmail.com;phone:+977-9845626345 location: Dang. Age: 28.\",\n",
        "    \"Hey there, this is Hari. You can reach me at 9841234567. I'm 19 and staying near Lalitpur.\",\n",
        "\n",
        "]\n",
        "\n",
        "for i, chat in enumerate(sample_chats, 1):\n",
        "    print(f\"\\n--- Sample chat #{i} ---\")\n",
        "    print(\"Input:\", chat)   # ðŸ‘ˆ added line to show input\n",
        "    out = extract_info_with_fallback(chat)\n",
        "    print(\"Source:\", out.get(\"from\"))\n",
        "    print(\"Parsed:\", out[\"parsed\"])\n",
        "    print(\"ValidAgainstSchema:\", out[\"valid\"])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G74sNas5iCaw",
        "outputId": "1040e72b-f850-4af0-de0d-97f38e07d73b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Sample chat #1 ---\n",
            "Input: Hi, I'm Isu Sharma. My email is isu.sharma@example.com and phone is +977-9812345678. I live in Kathmandu. I'm 22 years old.\n",
            "[retry_on_rate_limit] Rate limit detected. Attempt 1/4. Sleeping 1.0s and retrying...\n",
            "[retry_on_rate_limit] Rate limit detected. Attempt 2/4. Sleeping 2.0s and retrying...\n",
            "[retry_on_rate_limit] Rate limit detected. Attempt 3/4. Sleeping 4.0s and retrying...\n",
            "[extract_info_with_fallback] Insufficient quota detected â€” using local fallback extractor.\n",
            "Source: fallback\n",
            "Parsed: {'name': 'Isu Sharma', 'email': 'isu.sharma@example.com', 'phone': '+9779812345678', 'location': 'Kathmandu', 'age': 22}\n",
            "ValidAgainstSchema: True\n",
            "\n",
            "--- Sample chat #2 ---\n",
            "Input: Hello â€” name: Sita Sharma; contact: sita_sharma@gmail.com;phone:+977-9845626345 location: Dang. Age: 28.\n",
            "[retry_on_rate_limit] Rate limit detected. Attempt 1/4. Sleeping 1.0s and retrying...\n",
            "[retry_on_rate_limit] Rate limit detected. Attempt 2/4. Sleeping 2.1s and retrying...\n",
            "[retry_on_rate_limit] Rate limit detected. Attempt 3/4. Sleeping 4.2s and retrying...\n",
            "[extract_info_with_fallback] Insufficient quota detected â€” using local fallback extractor.\n",
            "Source: fallback\n",
            "Parsed: {'name': 'Sita Sharma', 'email': 'sita_sharma@gmail.com', 'phone': '+9779845626345', 'location': 'Dang', 'age': 28}\n",
            "ValidAgainstSchema: True\n",
            "\n",
            "--- Sample chat #3 ---\n",
            "Input: Hey there, this is Hari. You can reach me at 9841234567. I'm 19 and staying near Lalitpur.\n",
            "[retry_on_rate_limit] Rate limit detected. Attempt 1/4. Sleeping 1.0s and retrying...\n",
            "[retry_on_rate_limit] Rate limit detected. Attempt 2/4. Sleeping 2.1s and retrying...\n",
            "[retry_on_rate_limit] Rate limit detected. Attempt 3/4. Sleeping 4.2s and retrying...\n",
            "[extract_info_with_fallback] Insufficient quota detected â€” using local fallback extractor.\n",
            "Source: fallback\n",
            "Parsed: {'name': 'Hari', 'email': None, 'phone': '9841234567', 'location': 'Lalitpur', 'age': 19}\n",
            "ValidAgainstSchema: False\n"
          ]
        }
      ]
    }
  ]
}